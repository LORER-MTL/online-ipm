"""
Logarithmic Functions: Barrier vs Objective Analysis
===================================================

This module clarifies the crucial distinction between:
1. Logarithmic BARRIER functions (enforce constraints x > 0)
2. Logarithmic functions in the OBJECTIVE (part of problem formulation)

The mathematical properties and behavior are completely different!
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy.optimize import minimize
import cvxpy as cp

def analyze_log_in_objective():
    """
    Analyze problems with logarithmic functions in the objective.
    """
    print("=" * 70)
    print("LOGARITHMIC FUNCTIONS IN THE OBJECTIVE")
    print("=" * 70)
    
    print("\nExamples of convex problems with log in objective:")
    print()
    print("1. MAXIMUM LIKELIHOOD ESTIMATION:")
    print("   minimize  -âˆ‘ log p(x_i | Î¸)  (negative log-likelihood)")
    print("   subject to  constraints on Î¸")
    print()
    print("2. ENTROPY MAXIMIZATION:")
    print("   maximize  âˆ‘ x_i log(x_i)  (entropy)")
    print("   subject to  âˆ‘ x_i = 1, x_i â‰¥ 0")
    print()
    print("3. LOG-SUM-EXP APPROXIMATION:")
    print("   minimize  log(âˆ‘ exp(a_i^T x + b_i))")
    print("   subject to  Ax = b")
    print()
    print("4. GEOMETRIC PROGRAMMING:")
    print("   minimize  log(âˆ‘ exp(a_i^T x + b_i))")
    print("   subject to  log(âˆ‘ exp(c_j^T x + d_j)) â‰¤ 0")

def log_barrier_vs_log_objective():
    """
    Compare logarithmic barriers vs logarithmic objectives.
    """
    print("\n" + "=" * 70)
    print("BARRIER vs OBJECTIVE: KEY DIFFERENCES")
    print("=" * 70)
    
    print("\nðŸ“‹ LOGARITHMIC BARRIER (what we analyzed before):")
    print("   Purpose: ENFORCE CONSTRAINTS x > 0")
    print("   Form: Ï†(x) = -âˆ‘ log(x_i)")
    print("   Domain: {x : x > 0}")
    print("   Problem: minimize f(x) - Î¼âˆ‘log(x_i)  [barrier added to objective]")
    print("   Behavior: Ï†(x) â†’ âˆž as x_i â†’ 0âº (prevents constraint violation)")
    print("   Hessian: âˆ‡Â²Ï†(x) = diag(1/xâ‚Â², ..., 1/xâ‚™Â²) â†’ âˆž near boundary")
    print()
    print("ðŸ“Š LOGARITHMIC OBJECTIVE (your question):")
    print("   Purpose: PART OF PROBLEM FORMULATION")
    print("   Form: f(x) = Î± log(g(x)) for some function g(x)")
    print("   Domain: {x : g(x) > 0} (natural domain of log)")
    print("   Problem: minimize Î± log(g(x))  [log is the actual objective]")
    print("   Behavior: Depends on specific problem structure")
    print("   Hessian: Depends on g(x) and Î± (can be well-behaved)")

def demonstrate_log_objective_problems():
    """
    Demonstrate actual problems with log in objective.
    """
    print("\n" + "=" * 70)
    print("EXAMPLES: LOG IN OBJECTIVE (WELL-BEHAVED)")
    print("=" * 70)
    
    print("\n1. PORTFOLIO OPTIMIZATION WITH LOG UTILITY:")
    print("   maximize  âˆ‘ w_i log(R_i)  (log utility of returns)")
    print("   subject to  âˆ‘ w_i = 1, w_i â‰¥ 0")
    print()
    
    # Demonstrate with CVXPY
    n = 3  # 3 assets
    R = np.array([1.05, 1.08, 1.12])  # Expected returns
    w = cp.Variable(n)
    
    print("   Example with 3 assets:")
    print(f"   Expected returns: {R}")
    
    # Log utility objective (concave, so we maximize)
    objective = cp.Maximize(cp.sum(cp.multiply(w, cp.log(R))))
    constraints = [cp.sum(w) == 1, w >= 0]
    
    prob = cp.Problem(objective, constraints)
    prob.solve()
    
    if prob.status == cp.OPTIMAL:
        print(f"   Optimal weights: {w.value}")
        print(f"   Objective value: {prob.value:.4f}")
    
    print("\n2. MAXIMUM ENTROPY PROBLEM:")
    print("   minimize  âˆ‘ x_i log(x_i)  (negative entropy)")
    print("   subject to  Ax = b, x â‰¥ 0")
    print()
    
    # Entropy example
    n = 4
    A = np.array([[1, 1, 1, 1],     # Sum constraint
                  [1, 2, 3, 4]])    # Moment constraint  
    b = np.array([1, 2.5])
    
    x = cp.Variable(n)
    # For numerical stability, use entr function (x*log(x))
    objective = cp.Minimize(-cp.sum(cp.entr(x)))  # Maximize entropy
    constraints = [A @ x == b, x >= 0]
    
    prob = cp.Problem(objective, constraints)
    prob.solve()
    
    if prob.status == cp.OPTIMAL:
        print(f"   Optimal distribution: {x.value}")
        print(f"   Entropy: {-prob.value:.4f}")

def analyze_hessian_behavior():
    """
    Analyze Hessian behavior for log objectives vs barriers.
    """
    print("\n" + "=" * 70)
    print("HESSIAN BEHAVIOR COMPARISON")
    print("=" * 70)
    
    x = np.linspace(0.1, 5, 100)
    
    # Barrier: Ï†(x) = -log(x)
    barrier_second = 1.0 / (x**2)  # Ï†''(x) = 1/xÂ²
    
    # Objective examples
    # 1. f(x) = log(x) (concave)
    obj1_second = -1.0 / (x**2)    # f''(x) = -1/xÂ²
    
    # 2. f(x) = x log(x) (convex for x > 1/e)
    obj2_second = 1.0 / x          # f''(x) = 1/x
    
    # 3. f(x) = (log(x))Â² (can be convex or concave)
    obj3_second = (1 - 2*np.log(x)) / (x**2)  # f''(x) = (1-2log(x))/xÂ²
    
    print("Function                Second Derivative      Behavior as xâ†’0âº")
    print("-" * 65)
    print(f"Barrier: -log(x)       1/xÂ²                   â†’ +âˆž (prevents boundary)")
    print(f"Objective: log(x)      -1/xÂ²                  â†’ -âˆž (concave)")
    print(f"Objective: x log(x)    1/x                    â†’ +âˆž (but slower)")
    print(f"Objective: (log(x))Â²   (1-2log(x))/xÂ²        Depends on log(x)")
    print()
    print("KEY INSIGHT:")
    print("â€¢ Barrier second derivatives ALWAYS â†’ +âˆž (by design)")
    print("â€¢ Objective second derivatives can be well-behaved or bounded")
    print("â€¢ The domain and constraints determine feasible behavior")

def newton_method_analysis():
    """
    Analyze when Newton's method works for log objectives.
    """
    print("\n" + "=" * 70)
    print("NEWTON'S METHOD FOR LOG OBJECTIVES")
    print("=" * 70)
    
    print("\nWhen Newton's method works well for log objectives:")
    print()
    print("âœ… WELL-POSED CASES:")
    print("   1. BOUNDED DOMAIN: x âˆˆ [a, b] with a > 0")
    print("      â€¢ Hessian remains bounded away from singularities")
    print("      â€¢ Newton steps are well-defined")
    print()
    print("   2. REGULARIZED PROBLEMS:")
    print("      minimize Î± log(g(x)) + Î²||x||Â² (add regularization)")
    print("      â€¢ Regularization dominates near boundary")
    print("      â€¢ Hessian stays positive definite")
    print()
    print("   3. CONSTRAINED TO INTERIOR:")
    print("      minimize log(x) subject to x â‰¥ Î´ > 0")
    print("      â€¢ Never approach the logarithm singularity")
    print("      â€¢ Standard Newton analysis applies")
    print()
    print("   4. COMPOSITE FUNCTIONS:")
    print("      minimize log(1 + exp(Ax + b))")
    print("      â€¢ log-sum-exp is smooth everywhere")
    print("      â€¢ No singularities in practice")

def demonstrate_well_behaved_log_problem():
    """
    Solve a well-behaved problem with log in objective using Newton.
    """
    print("\n" + "=" * 70)
    print("EXAMPLE: WELL-BEHAVED LOG OBJECTIVE")
    print("=" * 70)
    
    print("\nProblem: minimize log(xâ‚ + xâ‚‚ + 1) + xâ‚Â² + xâ‚‚Â²")
    print("         subject to xâ‚, xâ‚‚ â‰¥ 0.1 (stay away from log singularity)")
    print()
    
    def objective(x):
        return np.log(x[0] + x[1] + 1) + x[0]**2 + x[1]**2
    
    def gradient(x):
        s = x[0] + x[1] + 1
        return np.array([1/s + 2*x[0], 1/s + 2*x[1]])
    
    def hessian(x):
        s = x[0] + x[1] + 1
        h11 = -1/(s**2) + 2
        h12 = -1/(s**2)
        h22 = -1/(s**2) + 2
        return np.array([[h11, h12], [h12, h22]])
    
    # Starting point
    x0 = np.array([1.0, 1.0])
    
    print(f"Starting point: xâ‚€ = {x0}")
    print(f"Initial objective: {objective(x0):.6f}")
    print()
    
    # Check Hessian properties
    H = hessian(x0)
    eigenvals = np.linalg.eigvals(H)
    
    print(f"Hessian at xâ‚€:")
    print(H)
    print(f"Eigenvalues: {eigenvals}")
    print(f"Positive definite: {np.all(eigenvals > 0)}")
    print()
    
    # Simple Newton iterations
    x = x0.copy()
    print("Newton iterations:")
    print("Iter    xâ‚        xâ‚‚        f(x)        ||âˆ‡f||")
    print("-" * 50)
    
    for k in range(5):
        f_val = objective(x)
        g = gradient(x)
        H = hessian(x)
        
        print(f"{k:3d}  {x[0]:8.4f} {x[1]:8.4f} {f_val:10.6f} {np.linalg.norm(g):10.2e}")
        
        # Newton step
        try:
            dx = -np.linalg.solve(H, g)
            x = x + dx
            
            # Ensure we stay in feasible region
            x = np.maximum(x, 0.1)
            
        except np.linalg.LinAlgError:
            print("Hessian became singular!")
            break
    
    print()
    print("âœ… Newton's method works well!")
    print("   â€¢ Hessian stays positive definite")
    print("   â€¢ Fast quadratic convergence")
    print("   â€¢ No singularities encountered")

def key_takeaways():
    """
    Summarize key insights about log functions.
    """
    print("\n" + "=" * 70)
    print("KEY TAKEAWAYS: WHEN LOG IN OBJECTIVE IS FINE")
    print("=" * 70)
    
    print("\nðŸŽ¯ MAIN INSIGHT:")
    print("   Logarithms in the OBJECTIVE are fundamentally different from")
    print("   logarithmic BARRIERS. The mathematical analysis is case-specific.")
    print()
    
    print("ðŸ“Š LOG OBJECTIVES ARE FINE WHEN:")
    print("   âœ… Domain is bounded away from log singularities")
    print("   âœ… Problem has natural regularization")
    print("   âœ… Constraints prevent approaching log(0)")
    print("   âœ… Using composite functions like log-sum-exp")
    print("   âœ… Hessian remains well-conditioned")
    print()
    
    print("âš ï¸  LOG OBJECTIVES ARE PROBLEMATIC WHEN:")
    print("   âŒ Unconstrained near log singularities")
    print("   âŒ Domain includes points where argument â†’ 0")
    print("   âŒ No regularization or constraints")
    print()
    
    print("ðŸ”§ BARRIER vs OBJECTIVE:")
    print("   â€¢ BARRIER: -log(x) ADDED to enforce x > 0 constraints")
    print("             Necessarily unbounded as x â†’ 0 (by design)")
    print("   â€¢ OBJECTIVE: log terms as PART of problem formulation")
    print("               Can be well-behaved with proper domain/constraints")
    print()
    
    print("ðŸ“š STANDARD APPLICATIONS:")
    print("   â€¢ Maximum likelihood estimation (well-posed with data)")
    print("   â€¢ Entropy optimization (constrained to probability simplex)")
    print("   â€¢ Portfolio optimization (returns bounded away from 0)")
    print("   â€¢ Log-sum-exp smoothing (no singularities)")
    print("   â€¢ Geometric programming (transformed to convex form)")

def main():
    """
    Main function explaining the distinction between barriers and objectives.
    """
    print("LOGARITHMIC FUNCTIONS: BARRIER vs OBJECTIVE ANALYSIS")
    print("Understanding when log in objective is mathematically sound")
    print("=" * 70)
    
    analyze_log_in_objective()
    log_barrier_vs_log_objective()
    demonstrate_log_objective_problems()
    analyze_hessian_behavior()
    newton_method_analysis()
    demonstrate_well_behaved_log_problem()
    key_takeaways()
    
    print("\n" + "=" * 70)
    print("FINAL ANSWER TO YOUR QUESTION")
    print("=" * 70)
    print()
    print("YES! You can absolutely use log in the objective for many problems.")
    print()
    print("The key distinction:")
    print("â€¢ LOG BARRIERS: Mathematical device to enforce constraints")
    print("  â†’ Must be unbounded (that's what makes them barriers)")
    print("  â†’ KKT matrices necessarily ill-conditioned near boundary")
    print()
    print("â€¢ LOG OBJECTIVES: Natural part of problem formulation")
    print("  â†’ Can be well-behaved with proper domain/constraints")
    print("  â†’ Newton's method works fine in many practical cases")
    print()
    print("The invalid Assumption 4 was about BARRIER problems specifically,")
    print("where the logarithmic terms are artificial additions for constraint")
    print("handling. Your observation about log objectives is completely correct!")

if __name__ == "__main__":
    main()